{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1261f23",
   "metadata": {},
   "source": [
    "# Locate possible outliers in Heave, North, and West data in .RDT files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e01be3f",
   "metadata": {},
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aec9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of packages (and their functions) used in the modules below\n",
    "using DataFrames: DataFrame, ncol, nrow\n",
    "using Dates: Day, Date, Dates, DateTime, Hour, Microsecond, Minute, Month, now, Time, unix2datetime, Year\n",
    "#using FilePathsBase\n",
    "using Flux: Adam, Chain, Dense, Flux, mse, params, relu, train!\n",
    "using Glob: glob\n",
    "using JLD2: @load, @save\n",
    "using NativeFileDialog: pick_file, pick_folder\n",
    "using Plots:  annotate!, font, hline!, hspan!, plot, Plots, plotly, plot!, scatter!, text, vline!, xlims, ylims, @layout\n",
    "using Printf: @sprintf\n",
    "using Sockets: gethostname\n",
    "using Statistics: mean, median, quantile, std\n",
    "using Tk: Button, Frame, Tk, Toplevel, Treeview, bind, destroy, get_value, pack, scrollbars_add, tcl\n",
    "\n",
    "include(\".\\\\Mk3_model_functions.jl\");    # this contains the functions called by the modules below\n",
    "\n",
    "println(\"Loading packages completed\")\n",
    "\n",
    "hostname = gethostname()\n",
    "##println(\"The name of the computer is: \", hostname)\n",
    "\n",
    "if hostname == \"QUEENSLAND-BASIN\"\n",
    "    \n",
    "    display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "    initial_path = \"E:\\\\Card Data\\\\\"\n",
    "    \n",
    "else\n",
    "    \n",
    "    display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))    \n",
    "    initial_path = \"F:\\\\Card Data\\\\\"\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae1cad-0139-439d-b48d-3249ad052f66",
   "metadata": {},
   "source": [
    "### Locate and display records with GPS errors as flagged by Datawell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9ef10-eae3-4c13-8be9-62f10dbbc1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using DataFrames: DataFrame\n",
    "using Dates: DateTime, year\n",
    "using DSP: welch_pgram, freq, power, hanning\n",
    "using Glob: glob\n",
    "using NativeFileDialog: pick_file\n",
    "\n",
    "import DataFrames: Not, select!\n",
    "\n",
    "# Widen screen for better viewing\n",
    "display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))\n",
    "\n",
    "# Helper function to convert data without creating temporary strings\n",
    "function parse_hex(data_array, idx)\n",
    "###################################\n",
    "    \n",
    "    UInt16(data_array[idx]) << 8 | UInt16(data_array[idx+1])\n",
    "    \n",
    "end    # parse_hex()\n",
    "\n",
    "\n",
    "# Function to check if the LSB is 1 (GPS interference)\n",
    "function check_gps_flag(north_row)\n",
    "##################################\n",
    "    \n",
    "    gps_flag_row = [((n & 0x1) == 1) ? 1 : 0 for n in north_row]\n",
    "    \n",
    "    return(gps_flag_row)\n",
    "    \n",
    "end    # check_gps_flag()\n",
    "    \n",
    "\n",
    "using Plots: plot!, vline\n",
    "using Dates: Microsecond, Minute\n",
    "\n",
    "# plot the selected displacement and GPS errors\n",
    "function plot_pp(xvals, errors, color, X_data, ii, num, tm_tick, ticks)\n",
    "#######################################################################\n",
    "    \n",
    "    pp = vline(xvals[errors], label=\"\", lc=:red, ls=:dot, ylims=extrema(X_data[:,ii,num]) .* 1.1)\n",
    "    pp = plot!(xvals, X_data[:,ii,num], lc=color, label=\"\", xlims=(xvals[1], xvals[end]), xticks=(tm_tick, ticks))\n",
    "\n",
    "    return(pp)\n",
    "\n",
    "end    # plot_pp()\n",
    "    \n",
    "\n",
    "# plot heave, north, and west displacements for selected date\n",
    "function do_plots(ii, X_date, X_data, GPS_errors)\n",
    "#################################################\n",
    "    \n",
    "    start_time = X_date[ii]\n",
    "    errors = findall(x -> x == 1, GPS_errors[:, ii, 1])\n",
    "\n",
    "    xvals = start_time + Microsecond.((0:REC_LENGTH-1) / SAMPLE_FREQUENCY * 1000000)\n",
    "\n",
    "    tm_tick = range(xvals[1], xvals[end], step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick, \"MM\")\n",
    "\n",
    "    title = Dates.format(X_date[ii], \"yyyy-mm-dd HH:MM\") * \" \" * string(length(errors)) * \" GPS errors\"\n",
    "\n",
    "    p1 = plot_pp(xvals, errors, :blue, X_data, ii, 1, tm_tick, ticks)\n",
    "    p2 = plot_pp(xvals, errors, :red, X_data, ii, 2, tm_tick, ticks)\n",
    "    p3 = plot_pp(xvals, errors, :green, X_data, ii, 3, tm_tick, ticks)\n",
    "    \n",
    "    plot_p1_p2_p3 = plot(p1, p2, p3, size=(2000, 1000), layout=(3,1), dpi=100, framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, \n",
    "        legend=:topright, xtickfont=font(8), ytickfont=font(8), bottommargin=5Plots.mm, suptitle=title, \n",
    "        yformatter = y -> @sprintf(\"%.1f\", y),\n",
    "        grid=true, gridlinewidth=0.125, gridstyle=:dot, gridcolor=:grey, gridalpha=0.5)\n",
    "    \n",
    "    display(plot_p1_p2_p3)\n",
    "\n",
    "end    # do_plots()\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "#######################################################################################################\n",
    "\n",
    "X_data = Matrix{Float32}(undef, 0, 0)\n",
    "# Initialize GPS_errors as an empty 3D array\n",
    "GPS_errors = Array{Int16}(undef, 2304, 0) #, 1)\n",
    "        \n",
    "infil = pick_file(initial_path)\n",
    "\n",
    "REC_LENGTH = 2304       # Number of WSE's in a Mk4 30-minute record\n",
    "SAMPLE_FREQUENCY = 1.28 # Mk4 sample frequency in Hertz\n",
    "SAMPLE_LENGTH = 1800    # Record length in seconds\n",
    "SAMPLE_RATE = Float64(1 / SAMPLE_FREQUENCY) # Sample spacing in seconds\n",
    "\n",
    "# Initialize output containers\n",
    "X_date = DateTime[]  # Vector to store timestamps for the selected file\n",
    "X_data = Float32[]   # Placeholder for water surface elevation data (Heave, North, West)\n",
    "\n",
    "@time begin\n",
    "    println(\"Reading BINARY data from \", infil)\n",
    "    flush(stdout)\n",
    "    \n",
    "    data_array = reinterpret(UInt8, read(infil))\n",
    "\n",
    "    ii = 1\n",
    "    num_records = 0  # Start counting records\n",
    "    while ii < length(data_array)\n",
    "        # Extract message header\n",
    "        message_length = (UInt16(data_array[ii + 2]) << 8) | UInt16(data_array[ii + 3])\n",
    "\n",
    "        # Extract timestamp\n",
    "        yr = (UInt16(data_array[ii + 5]) << 8) | UInt16(data_array[ii + 6])\n",
    "        month = data_array[ii + 7]\n",
    "        day = data_array[ii + 8]\n",
    "        hour = data_array[ii + 9]\n",
    "        minute = data_array[ii + 10]\n",
    "\n",
    "        # Store timestamp for the record\n",
    "        push!(X_date, DateTime(yr, month, day, hour, minute))\n",
    "\n",
    "        # Validate sample frequency\n",
    "        sample_rate_hex = UInt32(data_array[ii + 11]) << 24 | UInt32(data_array[ii + 12]) << 16 | \n",
    "                          UInt32(data_array[ii + 13]) << 8 | UInt32(data_array[ii + 14])\n",
    "        sample_frequency = reinterpret(Float32, sample_rate_hex)\n",
    "\n",
    "        if sample_frequency != 1.28f0\n",
    "            error(\"Error: Sample rate not 1.28 Hz - Program terminated!\")\n",
    "        end\n",
    "\n",
    "        rows = (message_length - 10) ÷ 6  # Calculate number of rows (samples)\n",
    "        if rows != REC_LENGTH\n",
    "            error(\"Error: Number of rows per record does not match expected sample count!\")\n",
    "        end\n",
    "\n",
    "        # Allocate temporary vectors for current record\n",
    "        heave_values = Vector{Float32}(undef, rows)\n",
    "        north_values = Vector{Float32}(undef, rows)\n",
    "        west_values = Vector{Float32}(undef, rows)\n",
    "        gps_error = Vector{Int16}(undef, rows)  # Temporary GPS error vector for the current record\n",
    "\n",
    "        for jj ∈ 1:rows\n",
    "            base_idx = ii + 15 + (jj - 1) * 6\n",
    "\n",
    "            # Parse HEX and calculate displacements\n",
    "            heave_values[jj] = reinterpret(Int16, UInt16(parse_hex(data_array, base_idx))) / 100\n",
    "            north_hex = parse_hex(data_array, base_idx + 2)\n",
    "            north_values[jj] = reinterpret(Int16, UInt16(north_hex)) / 100\n",
    "            west_values[jj] = reinterpret(Int16, UInt16(parse_hex(data_array, base_idx + 4))) / 100\n",
    "\n",
    "            # Determine GPS error from the least significant bit of the North value\n",
    "            gps_error[jj] = parse(Int, last(string(north_hex, base=2, pad=16), 1))  # LSB: 1 (error) or 0 (no error)\n",
    "        end\n",
    "\n",
    "        # Append the GPS error for the current record\n",
    "##        GPS_errors = cat(GPS_errors, reshape(gps_error, rows, 1, 1), dims=2)\n",
    "        GPS_errors = hcat(GPS_errors, gps_error)\n",
    "        \n",
    "        # Incrementally add record data to X_data\n",
    "        if isempty(X_data)\n",
    "            X_data = zeros(Float32, rows, 1, 3)  # Initialize with the first record\n",
    "        else\n",
    "            X_data = cat(X_data, reshape([heave_values north_values west_values], rows, 1, 3), dims=2)\n",
    "        end\n",
    "\n",
    "        # Update counters\n",
    "        num_records += 1\n",
    "        ii += message_length + 6\n",
    "    end\n",
    "\n",
    "    println(\"File processing complete: \", num_records, \" records processed.\")\n",
    "end\n",
    "\n",
    "# Locate GPS errors flagged by Datawell\n",
    "column_sums = sum(GPS_errors[:, :, 1], dims=1)  # Sum along the rows (dimension 1)\n",
    "\n",
    "column_sums_vector = vec(column_sums)  # Converts the 1×288 matrix to a 288-element vector\n",
    "records_with_errors = findall(x -> x > 0, column_sums_vector)\n",
    "\n",
    "println(\"\\n\",string(length(records_with_errors)),\" records with GPS errors: \")\n",
    "\n",
    "if isempty(records_with_errors)   \n",
    "    println(\"\\nNo GPS errors flagged by Datawell in \",infil)\n",
    "else\n",
    "    foreach(ii -> println(\"    \", Dates.format(X_date[ii], \"yyyy-mm-dd HH:MM\")), records_with_errors)\n",
    "    foreach(ii -> do_plots(ii, X_date, X_data, GPS_errors), records_with_errors)\n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate GPS errors flagged by Datawell\n",
    "column_sums = sum(GPS_errors[:, :, 1], dims=1)  # Sum along the rows (dimension 1)\n",
    "\n",
    "column_sums_vector = vec(column_sums)  # Converts the 1×288 matrix to a 288-element vector\n",
    "records_with_errors = findall(x -> x > 0, column_sums_vector)\n",
    "\n",
    "plot(X_data[:,66,1], label=\"\", size=(2000,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254b703-de9f-456f-85ee-66e9d34ba9ef",
   "metadata": {},
   "source": [
    "### Display spectra for records with GPS errors as flagged by Datawell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd8d55-fbb7-4f4b-9869-8163fd43777d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to calculate f2 and Pden2 using Welch's method\n",
    "function calculate_spectra(heave_row, sample_frequency)\n",
    "#######################################################\n",
    "    ps_w = welch_pgram(heave_row, 256, 128; onesided=true, nfft=256, fs=sample_frequency, window=hanning)\n",
    "    f2 = freq(ps_w)\n",
    "    Pden2 = power(ps_w)\n",
    "    return f2, Pden2\n",
    "end    # calculate_spectra()\n",
    "\n",
    "\n",
    "# Plot the spectra\n",
    "function plot_spectra(f2, Pden2, X_date)\n",
    "########################################\n",
    "    \n",
    "    title = Dates.format(X_date, \"yyyy-mm-dd HH:MM\")\n",
    "\n",
    "    p1 = plot(f2, Pden2, title=title, fillrange=:0, label=\"\", xlims=(0,0.64), ylims=(0,Inf), size=(1000,600), framestyle=:box)\n",
    "\n",
    "    display(p1)\n",
    "\n",
    "end    # plot_spectra()\n",
    "\n",
    "\n",
    "# Perform spectral calculations\n",
    "sample_frequency = 1.28f0\n",
    "num_records = length(X_date)  # Adjust as needed\n",
    "nfft = 256         # FFT size used in Welch's method\n",
    "n_bins = nfft ÷ 2 + 1  # Number of frequency bins (for onesided FFT)\n",
    "\n",
    "# Initialize 2D matrices\n",
    "f2_list = zeros(Float32, num_records, n_bins)\n",
    "Pden2_list = zeros(Float32, num_records, n_bins)\n",
    "\n",
    "for file_idx ∈ 1:num_records\n",
    "    heave_row = X_data[:, file_idx, 1]\n",
    "    f2, Pden2 = calculate_spectra(heave_row, sample_frequency)\n",
    "    f2_list[file_idx, :] = f2\n",
    "    Pden2_list[file_idx, :] = Pden2\n",
    "end\n",
    "\n",
    "if isempty(records_with_errors)   \n",
    "    println(\"\\nNo GPS errors flagged by Datawell in \",infil)\n",
    "else\n",
    "    foreach(ii -> println(\"    \", Dates.format(X_date[ii], \"yyyy-mm-dd HH:MM\")), records_with_errors)\n",
    "    foreach(ii -> plot_spectra(f2_list[ii, :], Pden2_list[ii, :], X_date[ii]), records_with_errors)\n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b5007-f8eb-4c9f-ab9e-3a8c8839f1af",
   "metadata": {},
   "source": [
    "### Display spectrogram for selected .RDT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598992a4-f63f-48c7-b54f-77b02e808454",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Spectral calculations complete - now plotting spectrogram!\")\n",
    "\n",
    "using Plots: contourf, cgrad\n",
    "x = X_date\n",
    "y = f2_list[1,:]\n",
    "z = Pden2_list'\n",
    "\n",
    "contourf(x, y, z, size=(1200,600))\n",
    "\n",
    "# display plots to screen\n",
    "tm_tick = range(X_date[1],X_date[end],step=Hour(4))\n",
    "ticks = Dates.format.(tm_tick,\"dd HH:MM\")\n",
    "\n",
    "p1 = contourf(x, y, z, lw=0.25, c=cgrad(:Spectral, rev=true), clims=(0.0,maximum(z)), levels=10, fill=true)\n",
    "\n",
    "# draw grid lines on plot\n",
    "for i in 0:0.1:0.6\n",
    "    hline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "end\n",
    "\n",
    "for i in X_date[1]:Hour(4):X_date[end]\n",
    "    vline!(p1, [i], lw=0.5, c=:white, label=\"\")\n",
    "end\n",
    "\n",
    "foreach(ii -> vline!([X_date[ii]], lw=1, ls=:dash, c=:yellow, label=\"\"), records_with_errors)\n",
    "\n",
    "title=Dates.format(X_date[1], \"yyyy-mm-dd HH:MM\")*\" to \"*Dates.format(X_date[end], \"yyyy-mm-dd HH:MM\")\n",
    "p1_plot = plot(p1, xlabel=\"Date\", xlim=(X_date[1],X_date[end]), xticks=(tm_tick,ticks), xtickfontsize=7, xrotation=90,\n",
    "        ylabel=\"Frequency (Hz)\", ylim=(0,0.4), ytickfontsize=8, \n",
    "        title=title, framestyle = :box,\n",
    "        leftmargin = 15Plots.mm, bottommargin = 15Plots.mm, grid=true, size=(2000,800), gridlinewidth=0.5, gridstyle=:dot, gridalpha=1, colorbar=true)\n",
    "\n",
    "display(p1_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e097e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function calc_and_plot_bounds(xvals, heave, lc, hspan_color, label, errors)\n",
    "###################################################################\n",
    "\n",
    "    Q1 = quantile(heave, 0.25)\n",
    "    Q3 = quantile(heave, 0.75)\n",
    "\n",
    "    multiplier = 1.5\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "\n",
    "    # Calculate dynamic confidence interval\n",
    "    confidence_interval = 3.29 # threshold at the 99.9th percentile level\n",
    "\n",
    "    # Identify z_scores using modified z-score\n",
    "    z_score_indices, mod_z_scores = modified_z_score(heave, confidence_interval)\n",
    "\n",
    "    # Plot confidence limits\n",
    "    confidence_limits = calc_confidence_limits(heave, confidence_interval)\n",
    "\n",
    "    tm_tick = range(xvals[1], xvals[end], step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick, \"MM\")\n",
    "\n",
    "\n",
    "    px = plot(xvals, heave, xlims=(xvals[1], xvals[end]), lw=0.5, lc=lc, alpha=0.5, \n",
    "        xticks=(tm_tick, ticks), label=label, legendfontsize=12)\n",
    "\n",
    "    px = vline!(xvals[errors], label=\"\", lc=:red, ls=:dot)\n",
    "\n",
    "    if !isempty(z_score_indices)\n",
    "        scatter!(px, xvals[z_score_indices], heave[z_score_indices], \n",
    "            markersize=4, markerstrokecolor=:red, markerstrokewidth=1, \n",
    "            markercolor=:white, markershape=:circle, label=\"Modified Z-score beyond 99.9% confidence limits\")\n",
    "    end\n",
    "\n",
    "    px = hspan!([lower_bound, upper_bound], fillcolor=hspan_color, fillalpha=:0.125, label=\"IQR limits\")\n",
    "    px = hline!([confidence_limits[1], confidence_limits[2]], color=:red, lw=0.5, linestyle=:dash, label=\"99.9% confidence limits\")            \n",
    "    \n",
    "    return(px)\n",
    "\n",
    "end    # calc_and_plot_bounds()\n",
    "\n",
    "\n",
    "function do_heave_north_west_plots(ii, start_time, X_data, GPS_errors)\n",
    "##########################################################\n",
    "\n",
    "    heave = X_data[:, ii, 1]\n",
    "    north = X_data[:, ii, 2]\n",
    "    west = X_data[:, ii, 3]\n",
    "\n",
    "    errors = findall(x -> x == 1, GPS_errors[:, ii, 1])\n",
    "    \n",
    "    end_time = start_time + Minute(30)\n",
    "    xvals = start_time + Microsecond.((0:REC_LENGTH-1) / SAMPLE_FREQUENCY * 1000000)\n",
    "   \n",
    "    tm_tick = range(start_time, end_time, step=Minute(1))\n",
    "    ticks = Dates.format.(tm_tick, \"MM\")\n",
    "            \n",
    "    p1 = calc_and_plot_bounds(xvals, heave, :blue, :lightblue, \"Heave\", errors)\n",
    "\n",
    "    p2 = calc_and_plot_bounds(xvals, north, :red, :pink, \"North\", errors)\n",
    "\n",
    "    p3 = calc_and_plot_bounds(xvals, west, :green, :lightgreen, \"West\", errors)\n",
    "\n",
    "    # Annotate plot with the number of outliers and confidence interval\n",
    "##    num_outliers = length(z_score_indices)\n",
    "##    suspect_string = string(\"  \", string(ii),\" \",Dates.format(start_time, \"yyyy-mm-dd HH:MM\"), \" - \", num_outliers, \" Possible outliers\") # using Confidence Interval of \", \n",
    "##        @sprintf(\"%.2f\", confidence_interval))\n",
    "##    annotate!(p1, xvals[1], maximum(heave) * 0.9, text(suspect_string, :left, 10, :blue))\n",
    "\n",
    "    date_string = Dates.format(start_time, \"yyyy-mm-dd HH:MM\") * \" \" * string(length(errors)) * \" GPS errors\"\n",
    " \n",
    "    plot_displacements = plot(p1, p2, p3, size=(2000, 1000), layout=(3,1), dpi=100, framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, \n",
    "    legend=:topright, xtickfont=font(8), ytickfont=font(8), bottommargin=5Plots.mm, suptitle=date_string,\n",
    "    grid=true, gridlinewidth=0.125, gridstyle=:dot, gridcolor=:grey, gridalpha=0.5)\n",
    " \n",
    "    display(plot_displacements)\n",
    "    \n",
    "end    # do_heave_north_west_plots()  \n",
    "\n",
    "for ii ∈ records_with_errors\n",
    "\n",
    "    # Initialize variables\n",
    "    start_time = X_date[ii]\n",
    "       \n",
    "    do_heave_north_west_plots(ii, start_time, X_data, GPS_errors)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9f67b-a526-4745-baee-b12fd8e4620b",
   "metadata": {},
   "source": [
    "### Initial declaration of training data matricies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94b42051-5d96-4a6e-a889-3ea960f30bc8",
   "metadata": {},
   "source": [
    "# Initialize empty 2D matrices with 2304 rows and 0 columns\n",
    "training_data_good = zeros(Float32, 2304, 0)\n",
    "training_data_bad = zeros(Float32, 2304, 0)\n",
    "\n",
    "# Check the results\n",
    "println(size(training_data_good))  # Should print (2304, 20)\n",
    "println(size(training_data_bad))  # Should print (2304, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188ff9e-dbb8-4305-9686-dad7b28f6e8a",
   "metadata": {},
   "source": [
    "### Build training data matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74224b-2585-4f84-b74b-1a8e99b34b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_with_errors = records_with_errors[2:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e452376-e4df-4e5a-9a44-9dbe558e2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of records with GPS errors in them\n",
    "new_training_data_bad = X_data[:,records_with_errors,1]\n",
    "println(string(length(records_with_errors)),\" records stored to training_data_bad matrix\")\n",
    "\n",
    "# Find indices of good records (not in records_with_errors)\n",
    "all_records = 1:size(X_data, 2)  # All column indices of X_data\n",
    "records_without_errors = setdiff(all_records, records_with_errors)\n",
    "\n",
    "# Create training_data_good\n",
    "new_training_data_good = X_data[:, records_without_errors, 1];\n",
    "println(string(length(records_without_errors)),\" records stored to training_data_good matrix\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f7fde-929d-4a92-8242-10dcb1c6ba44",
   "metadata": {},
   "source": [
    "### Add new training data to existing matricies and save to .JLD2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd5dd7-1708-45ef-8042-886bb5e37234",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2, Dates\n",
    "\n",
    "##training_data_good = hcat(training_data_good, new_training_data_good)\n",
    "training_data_bad = hcat(training_data_bad, new_training_data_bad)\n",
    "\n",
    "println(\"Good data now \",string(size(training_data_good)[2]),\" records\")\n",
    "println(\"Bad  data now \",string(size(training_data_bad)[2]),\" records\\n\")\n",
    "\n",
    "outfil = \".\\\\Training_data\\\\Mk3_training_data_updated_\" * Dates.format(now(), \"yyyy_mm_dd_HHMM\") * \".JLD2\" \n",
    "\n",
    "# Save the updated good and bad training data\n",
    "@save outfil training_data_good training_data_bad\n",
    "\n",
    "println(\"Updated training data saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487829b3",
   "metadata": {},
   "source": [
    "# *******************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b6edd-0a23-4c79-96c2-049e081fb686",
   "metadata": {},
   "source": [
    "### Recover earlier separated training data from file (Note: does not include Model data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497d8d3-af9f-4640-9241-21a8a1e166b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and optimizer states from the JLD2 file\n",
    "current_path = pwd() * \"\\\\Training_data\"\n",
    "filterlist = \"JLD2\"\n",
    "infil = pick_file(current_path; filterlist)\n",
    "\n",
    "# Load all saved training data\n",
    "println(\"\\nLoading training data from \",infil)\n",
    "flush(stdout)    \n",
    "@load infil training_data_good training_data_bad training_data_bad\n",
    "\n",
    "println(\"Data and labels loaded successfully.\")\n",
    "println(\"Good data contains \",string(size(training_data_good)[2]),\" records\")\n",
    "println(\"Bad  data contains \",string(size(training_data_bad)[2]),\" records\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_good # = training_data_good[:,2:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4449a423-e9a5-4b58-ba9c-6052338e993a",
   "metadata": {},
   "source": [
    "### Build the Model using a hybrid approach¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd4c43-0d8c-4489-982f-517497e6de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==\n",
    "Calls:\n",
    "\n",
    "    min_max_normalize_matrix()\n",
    "\n",
    "==#\n",
    "\n",
    "# Define autoencoder model\n",
    "hybrid_model = Chain(\n",
    "    Dense(2304, 256, relu),\n",
    "    Dense(256, 128, relu),\n",
    "    Dense(128, 32, relu),\n",
    "    Dense(32, 128, relu),\n",
    "    Dense(128, 256, relu),\n",
    "    Dense(256, 2304)\n",
    ")\n",
    "\n",
    "# Concatenate and normalize the training data\n",
    "training_data_combined = hcat(training_data_good, training_data_bad)\n",
    "training_data_normalized = min_max_normalize_matrix(training_data_combined)\n",
    "training_data_float32 = Float32.(training_data_normalized)\n",
    "\n",
    "# approx. computer-specific model build time\n",
    "if hostname == \"QUEENSLAND-BASIN\"  \n",
    "    println(\"Building hybrid model now - on this computer it takes about 30s\\n\")\n",
    "else   \n",
    "    println(\"Building hybrid model now - on this computer it takes about 200s\\n\")\n",
    "end   \n",
    "\n",
    "flush(stdout)  \n",
    "\n",
    "@time begin\n",
    "    \n",
    "    # Train the model\n",
    "    loss(x) = Flux.mse(hybrid_model(x), x)\n",
    "    opt = Adam()\n",
    "    \n",
    "    data = Iterators.repeated((training_data_float32,), 100)\n",
    "    Flux.train!(loss, Flux.params(hybrid_model), data, opt)\n",
    "    println(\"Model training complete.\")\n",
    "\n",
    "end\n",
    "\n",
    "println(\"\\nNow select a .RDT file to check for outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec265806-910f-4fbb-92f6-72c196652bf0",
   "metadata": {},
   "source": [
    "### Select a .RDT file to check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc40b9f-dab3-4d3d-a664-790db6b86adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert data without creating temporary strings\n",
    "function parse_hex(data_array, idx)\n",
    "###################################\n",
    "    \n",
    "    UInt16(data_array[idx]) << 8 | UInt16(data_array[idx+1])\n",
    "    \n",
    "end    # parse_hex()\n",
    "\n",
    "\n",
    "# Function to check if the LSB is 1 (GPS interference)\n",
    "function check_gps_flag(north_row)\n",
    "##################################\n",
    "    \n",
    "    gps_flag_row = [((n & 0x1) == 1) ? 1 : 0 for n in north_row]\n",
    "    \n",
    "    return(gps_flag_row)\n",
    "    \n",
    "end    # check_gps_flag()\n",
    "\n",
    "\n",
    "hostname = gethostname()\n",
    "##println(\"The name of the computer is: \", hostname)\n",
    "\n",
    "if hostname == \"QUEENSLAND-BASIN\"\n",
    "    \n",
    "    display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")\n",
    "    initial_path = \"E:\\\\Card Data\\\\\"\n",
    "    \n",
    "else\n",
    "    \n",
    "    display(HTML(\"<style>.jp-Cell { width: 120% !important; }</style>\"))    \n",
    "    initial_path = \"F:\\\\Card Data\\\\\"\n",
    "    \n",
    "end\n",
    "\n",
    "X_data = Matrix{Float32}(undef, 0, 0)\n",
    "# Initialize GPS_errors as an empty 3D array\n",
    "GPS_errors = Array{Int16}(undef, 2304, 0) #, 1)\n",
    "        \n",
    "infil = pick_file(initial_path)\n",
    "\n",
    "REC_LENGTH = 2304       # Number of WSE's in a Mk4 30-minute record\n",
    "SAMPLE_FREQUENCY = 1.28 # Mk4 sample frequency in Hertz\n",
    "SAMPLE_LENGTH = 1800    # Record length in seconds\n",
    "SAMPLE_RATE = Float64(1 / SAMPLE_FREQUENCY) # Sample spacing in seconds\n",
    "\n",
    "# Initialize output containers\n",
    "X_date = DateTime[]  # Vector to store timestamps for the selected file\n",
    "X_data = Float32[]   # Placeholder for water surface elevation data (Heave, North, West)\n",
    "\n",
    "@time begin\n",
    "    println(\"Reading BINARY data from \", infil)\n",
    "    flush(stdout)\n",
    "    \n",
    "    data_array = reinterpret(UInt8, read(infil))\n",
    "\n",
    "    ii = 1\n",
    "    num_records = 0  # Start counting records\n",
    "    while ii < length(data_array)\n",
    "        # Extract message header\n",
    "        message_length = (UInt16(data_array[ii + 2]) << 8) | UInt16(data_array[ii + 3])\n",
    "\n",
    "        # Extract timestamp\n",
    "        yr = (UInt16(data_array[ii + 5]) << 8) | UInt16(data_array[ii + 6])\n",
    "        month = data_array[ii + 7]\n",
    "        day = data_array[ii + 8]\n",
    "        hour = data_array[ii + 9]\n",
    "        minute = data_array[ii + 10]\n",
    "\n",
    "        # Store timestamp for the record\n",
    "        push!(X_date, DateTime(yr, month, day, hour, minute))\n",
    "\n",
    "        # Validate sample frequency\n",
    "        sample_rate_hex = UInt32(data_array[ii + 11]) << 24 | UInt32(data_array[ii + 12]) << 16 | \n",
    "                          UInt32(data_array[ii + 13]) << 8 | UInt32(data_array[ii + 14])\n",
    "        sample_frequency = reinterpret(Float32, sample_rate_hex)\n",
    "\n",
    "        if sample_frequency != 1.28f0\n",
    "            error(\"Error: Sample rate not 1.28 Hz - Program terminated!\")\n",
    "        end\n",
    "\n",
    "        rows = (message_length - 10) ÷ 6  # Calculate number of rows (samples)\n",
    "        if rows != REC_LENGTH\n",
    "            error(\"Error: Number of rows per record does not match expected sample count!\")\n",
    "        end\n",
    "\n",
    "        # Allocate temporary vectors for current record\n",
    "        heave_values = Vector{Float32}(undef, rows)\n",
    "        north_values = Vector{Float32}(undef, rows)\n",
    "        west_values = Vector{Float32}(undef, rows)\n",
    "        gps_error = Vector{Int16}(undef, rows)  # Temporary GPS error vector for the current record\n",
    "\n",
    "        for jj ∈ 1:rows\n",
    "            base_idx = ii + 15 + (jj - 1) * 6\n",
    "\n",
    "            # Parse HEX and calculate displacements\n",
    "            heave_values[jj] = reinterpret(Int16, UInt16(parse_hex(data_array, base_idx))) / 100\n",
    "            north_hex = parse_hex(data_array, base_idx + 2)\n",
    "            north_values[jj] = reinterpret(Int16, UInt16(north_hex)) / 100\n",
    "            west_values[jj] = reinterpret(Int16, UInt16(parse_hex(data_array, base_idx + 4))) / 100\n",
    "\n",
    "            # Determine GPS error from the least significant bit of the North value\n",
    "            gps_error[jj] = parse(Int, last(string(north_hex, base=2, pad=16), 1))  # LSB: 1 (error) or 0 (no error)\n",
    "        end\n",
    "\n",
    "        # Append the GPS error for the current record\n",
    "##        GPS_errors = cat(GPS_errors, reshape(gps_error, rows, 1, 1), dims=2)\n",
    "        GPS_errors = hcat(GPS_errors, gps_error)\n",
    "        \n",
    "        # Incrementally add record data to X_data\n",
    "        if isempty(X_data)\n",
    "            X_data = zeros(Float32, rows, 1, 3)  # Initialize with the first record\n",
    "        else\n",
    "            X_data = cat(X_data, reshape([heave_values north_values west_values], rows, 1, 3), dims=2)\n",
    "        end\n",
    "\n",
    "        # Update counters\n",
    "        num_records += 1\n",
    "        ii += message_length + 6\n",
    "    end\n",
    "\n",
    "    println(\"File processing complete: \", num_records, \" records processed.\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7bd1b9-34ab-4400-a1ff-9aee0be7800f",
   "metadata": {},
   "source": [
    "### Run the hybrid model against data in the selected .RDT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae206bf-0d63-4b90-99d4-67040fb78062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#==\n",
    "Calls:\n",
    "\n",
    "    detect_outliers()\n",
    "\n",
    "==#\n",
    "\n",
    "@time begin\n",
    "    \n",
    "    # identify possible outliers in Heave, North, and West data\n",
    "    outlier_heave, uncertain_heave, outlier_dates_heave, uncertain_dates_heave, good_thresh_heave, bad_thresh_heave = \n",
    "        detect_outliers(X_data[:,:,1], X_date, training_data_good, training_data_bad, hybrid_model)\n",
    "    outlier_north, uncertain_north, outlier_dates_north, uncertain_dates_north, good_thresh_north, bad_thresh_north = \n",
    "        detect_outliers(X_data[:,:,2], X_date, training_data_good, training_data_bad, hybrid_model)\n",
    "    outlier_west, uncertain_west, outlier_dates_west, uncertain_dates_west, good_thresh_west, bad_thresh_west = \n",
    "        detect_outliers(X_data[:,:,3], X_date, training_data_good, training_data_bad, hybrid_model)\n",
    "    \n",
    "    # Combine and deduplicate dates across components\n",
    "    all_outlier = unique(vcat(outlier_heave, outlier_north, outlier_west))\n",
    "    all_uncertain = unique(vcat(uncertain_heave, uncertain_north, uncertain_west))\n",
    "    \n",
    "    # Combine and deduplicate dates across components\n",
    "    all_outlier_dates = unique(vcat(outlier_dates_heave, outlier_dates_north, outlier_dates_west))\n",
    "    all_uncertain_dates = unique(vcat(uncertain_dates_heave, uncertain_dates_north, uncertain_dates_west));\n",
    "\n",
    "end\n",
    "\n",
    "# Output results\n",
    "println(\"\\nFor \",infil,\"\\n\")\n",
    "if !isempty(all_outlier_dates)\n",
    "    println(string(length(all_outlier_dates)), \" records contain suspected outliers at the following dates:\\n\")\n",
    "    for date in all_outlier_dates\n",
    "        println(\"    \", Dates.format(date, \"yyyy-mm-dd HH:MM\"))\n",
    "    end\n",
    "    print(\"\\n\")\n",
    "else\n",
    "    println(\"No suspected outliers detected.\\n\")\n",
    "end\n",
    "\n",
    "if !isempty(all_uncertain_dates)\n",
    "    println(string(length(all_uncertain_dates)), \" records contain uncertain data points at the following dates:\\n\")\n",
    "    for date in all_uncertain_dates\n",
    "        println(\"    \", Dates.format(date, \"yyyy-mm-dd HH:MM\"))\n",
    "    end\n",
    "else\n",
    "    println(\"No uncertain data points detected.\")\n",
    "end\n",
    "\n",
    "println(\"\\nNow run the plot routine to view the suspect records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8a39e-d215-4046-b6c9-6a189a9bf8c9",
   "metadata": {},
   "source": [
    "### Plot records with suspect data (as identified by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca8eb1-4b13-450b-ba45-c4da911dfffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#==\n",
    "Calls:\n",
    "\n",
    "    do_heave_north_west_plots()\n",
    "\n",
    "==#\n",
    "\n",
    "combined_outlier = unique(vcat(all_outlier, records_with_errors))\n",
    "\n",
    "jj = 1\n",
    "\n",
    "for ii ∈ sort(combined_outlier) # all_outlier)\n",
    "\n",
    "    # Initialize variables\n",
    "    start_time = X_date[ii]\n",
    "       \n",
    "    do_heave_north_west_plots(ii, start_time, X_data, GPS_errors)\n",
    "\n",
    "    jj += 1\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fa533",
   "metadata": {},
   "source": [
    "### Select new bad-data to be added to training_data_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f6ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_data_bad = zeros(Float32, 2304, 0)\n",
    "\n",
    "# Enter indicies of bad data from plots\n",
    "bad_data_indices = [3] # <----- Change these values!\n",
    "\n",
    "for ii in bad_data_indices\n",
    "    \n",
    "    kk = all_outlier[ii]\n",
    "    \n",
    "    new_column = reshape(X_data[:, kk, 1], :, 1)  # Ensure it’s a column vector\n",
    "    new_training_data_bad = hcat(new_training_data_bad, new_column)\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0344cb",
   "metadata": {},
   "source": [
    "### Update the training_data_bad and create an updated training data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f92078",
   "metadata": {},
   "outputs": [],
   "source": [
    "##training_data_good = hcat(training_data_good, new_training_data_good)\n",
    "training_data_bad = hcat(training_data_bad, new_training_data_bad)\n",
    "\n",
    "println(\"Good data now \",string(size(training_data_good)[2]),\" records\")\n",
    "println(\"Bad  data now \",string(size(training_data_bad)[2]),\" records\\n\")\n",
    "\n",
    "outfil = \".\\\\Training_data\\\\Mk3_training_data_updated_\" * Dates.format(now(), \"yyyy_mm_dd_HHMM\") * \".JLD2\" \n",
    "\n",
    "# Save the updated good and bad training data\n",
    "@save outfil training_data_good training_data_bad\n",
    "\n",
    "println(\"Updated bad training data saved successfully.\")\n",
    "println(\"NOTE: you will need to download the new training data file and build the model again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74e059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ii in bad_data_indices\n",
    "    \n",
    "    kk = all_outlier[ii]\n",
    "    \n",
    "    start_time = X_date[kk]\n",
    "    p1 = plot!(X_data[:,kk,1], lc=:blue, alpha=:0.5, label=string(\"Heave\"))\n",
    "    end_time = start_time + Minute(30)\n",
    "    xvals = start_time + Microsecond.((0:REC_LENGTH-1) / SAMPLE_FREQUENCY * 1000000)\n",
    "\n",
    "    p1 = plot(size=(2500,300), framestyle=:box, fg_legend=:transparent, bg_legend=:transparent, )\n",
    "    errors = findall(x -> x == 1, GPS_errors[:, all_outlier[ii], 1])\n",
    "    \n",
    "    p1 = vline!([errors], lc=:red, alpha=:0.5, label=\"\")\n",
    "    \n",
    "    p1 = plot!(X_data[:,kk,1], lc=:blue, alpha=:0.5, label=string(X_date[kk]))\n",
    "    \n",
    "    display(p1)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a2f79",
   "metadata": {},
   "source": [
    "### Save training data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beacedc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfil = \".\\\\Training_data\\\\Mk3_training_data_updated_\" * Dates.format(now(), \"yyyy_mm_dd_HHMM\") * \".JLD2\" \n",
    "\n",
    "# Save the updated good and bad training data\n",
    "@save outfil training_data_good training_data_bad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca72f66-c8ee-4db1-b406-273aca0a3e99",
   "metadata": {},
   "source": [
    "### Select .RDT directory and read its files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a3f38-a482-4272-9699-7e24f27c3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==\n",
    "Calls:\n",
    "\n",
    "    get_sorted_file_data()\n",
    "    plot_all_directory()\n",
    "\n",
    "==#\n",
    "\n",
    "# Define the path to the directory you want to search in\n",
    "directory_path = pick_folder(initial_path)\n",
    "\n",
    "# Use glob to find all .RDT files in the directory and subdirectories\n",
    "println(\"Reading all .RDT files in \",directory_path)\n",
    "flush(stdout)\n",
    "rdt_files = glob(\".//*.RDT\", directory_path)\n",
    "\n",
    "# remove the TMP.RDT file from the array\n",
    "TMP_file = \"TMP.RDT\"\n",
    "rdt_files = filter(file -> basename(file) != TMP_file, rdt_files)\n",
    "\n",
    "# Extract sorted dates, Hmax_values, and rdt_files\n",
    "sorted_dates, sorted_rdt_files, sorted_Hsig_values = get_sorted_file_data(infil, rdt_files)\n",
    "date_array = Dates.format.(values(sorted_dates), \"yyyy-mm-dd\");\n",
    "  \n",
    "plot_all_directory(sorted_dates, sorted_Hsig_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818735c8-58b7-49fb-b9a5-ed6f0d069fd5",
   "metadata": {},
   "source": [
    "### Select a .RDT file from menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b8766-c4f7-47b0-b84d-648dc389ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==\n",
    "Calls:\n",
    "\n",
    "    select_date_from_list()\n",
    "\n",
    "==#\n",
    "\n",
    "dates_array = Dates.format.(sorted_dates, \"yyyy-mm-dd\")\n",
    "\n",
    "selected_date = select_date_from_list(dates_array)\n",
    "println(\"Selected date: \", selected_date === nothing ? \"None\" : selected_date)\n",
    "\n",
    "index = findall(x -> x == selected_date,string.(sorted_dates));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e303750-cbc9-4a1d-ac5e-f416869f6bc1",
   "metadata": {},
   "source": [
    "### Read the selected .RDT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ac736-6f36-4e14-b89a-cf1e34291c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "infil = directory_path * \"\\\\\" * sorted_rdt_files[index[1]]\n",
    "\n",
    "# Initialize GPS_errors as an empty 3D array\n",
    "        \n",
    "REC_LENGTH = 2304       # Number of WSE's in a Mk4 30-minute record\n",
    "SAMPLE_FREQUENCY = 1.28 # Mk4 sample frequency in Hertz\n",
    "SAMPLE_LENGTH = 1800    # Record length in seconds\n",
    "SAMPLE_RATE = Float64(1 / SAMPLE_FREQUENCY) # Sample spacing in seconds\n",
    "\n",
    "X_data, X_date, GPS_errors = decode_rdt_data(infil)\n",
    "\n",
    "# Identify zero columns and remove them\n",
    "zero_columns = findall(i -> all(==(0), X_data[:, i, :]), 1:size(X_data, 2))\n",
    "##X_data = X_data[:, setdiff(1:size(X_data, 2), zero_columns), :]\n",
    "##X_date = X_date[setdiff(1:length(X_date), zero_columns)]\n",
    "\n",
    "# Locate GPS errors flagged by Datawell\n",
    "column_sums = sum(GPS_errors[:, :, 1], dims=1)  # Sum along the rows (dimension 1)\n",
    "\n",
    "column_sums_vector = vec(column_sums)  # Converts the 1×288 matrix to a 288-element vector\n",
    "records_with_errors = findall(x -> x > 0, column_sums_vector)\n",
    "\n",
    "println(\"\\n\",string(length(records_with_errors)),\" records with GPS errors: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b16c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_date[records_with_errors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315df2e-3792-4ab8-9646-b294e2531b48",
   "metadata": {},
   "source": [
    "### Plot suspected outlier records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbc610-a55b-4cc6-83cc-8f890ee76363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ii ∈ records_with_errors\n",
    "\n",
    "    # Initialize variables\n",
    "    start_time = X_date[ii]\n",
    "       \n",
    "    do_heave_north_west_plots(ii, start_time, X_data, GPS_errors)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75e650-78e0-4516-9f1f-225c58d5b137",
   "metadata": {},
   "source": [
    "### Save model, optimiser, and training data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f3894-952b-41c3-9803-a2b25e201929",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, JLD2, Dates\n",
    "\n",
    "# Save the model state, optimizer state, and relevant data/labels\n",
    "outfil = \".\\\\Model\\\\RDT_model_and_data_\"*Dates.format(now(), \"yyyy_mm_dd_HHMM\")*\".JLD2\" \n",
    "\n",
    "# Save model and optimizer and normalised wave data to a JLD2 file\n",
    "@save outfil hybrid_model opt training_data_good training_data_bad\n",
    "\n",
    "println(\"Data and model saved successfully to \",outfil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11958040-d259-4dbb-b2b8-57f54199099a",
   "metadata": {},
   "source": [
    "### Recover .RDT hybrid_model, optimiser, and training data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfcb70-2c90-4246-9da1-b768d4739ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2, Flux\n",
    "using NativeFileDialog: pick_file\n",
    "\n",
    "# Select the JLD2 file\n",
    "current_path = pwd() * \"\\\\Model\"\n",
    "filterlist = \"JLD2\"\n",
    "infil = pick_file(current_path; filterlist)\n",
    "\n",
    "# Ensure the file extension is uppercase\n",
    "infil = replace(infil, \".jld2\" => \".JLD2\")\n",
    "\n",
    "println(\"File selected: $infil\")\n",
    "\n",
    "# Verify keys in the JLD2 file\n",
    "JLD2.jldopen(infil, \"r\") do file\n",
    "    println(\"Keys in saved file:\", keys(file))\n",
    "end\n",
    "\n",
    "# Load the saved data into variables\n",
    "@load infil hybrid_model opt training_data_good training_data_bad\n",
    "\n",
    "# Verify the loaded data\n",
    "println(\"Model type: \", typeof(hybrid_model))         # Should be Chain\n",
    "println(\"Optimizer type: \", typeof(opt))             # Should be an optimizer, e.g., Adam\n",
    "println(\"Good training data type: \", typeof(training_data_good))  # Should be Array\n",
    "println(\"Bad training data type: \", typeof(training_data_bad))    # Should be Array\n",
    "\n",
    "println(\"Model and training data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b4b91d-1920-4564-a196-35d356fec131",
   "metadata": {},
   "source": [
    "### Update hybrid_model and optimiser by adding new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47663e9c-8317-41ed-870c-80586707dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, IterTools\n",
    "\n",
    "\n",
    "Combine existing and new training data\n",
    "updated_training_data_good = hcat(training_data_good, new_training_data_good)\n",
    "updated_training_data_bad = hcat(training_data_bad, new_training_data_bad)\n",
    "\n",
    "Concatenate good and bad training data\n",
    "updated_training_data_combined = hcat(updated_training_data_good, updated_training_data_bad)\n",
    "\n",
    "Normalize the updated training data\n",
    "updated_training_data_normalized = min_max_normalize_matrix(updated_training_data_combined)\n",
    "updated_training_data_float32 = Float32.(updated_training_data_normalized)\n",
    "\n",
    "Retrain the model with the updated data\n",
    "println(\"Updating the model with new training data...\")\n",
    "\n",
    "loss(x) = Flux.mse(hybrid_model(x), x)\n",
    "data = Iterators.repeated((updated_training_data_float32,), 100)  # Number of epochs\n",
    "Flux.train!(loss, Flux.params(hybrid_model), data, opt)\n",
    "\n",
    "println(\"Model updated with new training data!\")\n",
    "\n",
    "# 5. Save the updated model, optimizer, and training data\n",
    "@save infil hybrid_model opt updated_training_data_good updated_training_data_bad\n",
    "println(\"Updated model and data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361db91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
